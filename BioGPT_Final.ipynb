{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Start NLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems                                               normal\n",
      "findings    The cardiac silhouette and mediastinum size ar...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# print(data.columns)\n",
    "# Extract 'problems' and 'findings' columns\n",
    "df= pd.read_csv(\"indiana_reports.csv\")[[\"Problems\",\"findings\"]]\n",
    "#print df train element by element\n",
    "print(df.iloc[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n",
    "#Also print gpu name\n",
    "if device == 'cuda':\n",
    "    print(cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_username(text):\n",
    "    url = re.compile(r'@[A-Za-z0-9_]+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def pre_process_text(text):\n",
    "    text = remove_URL(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_username(text)\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['findings'], inplace=True)  # Remove rows with NaN in 'findings'\n",
    "df = df[df['findings'].str.strip().astype(bool)]  # Remove rows with empty or whitespace 'findings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems\n",
      "normal                                                      1197\n",
      "No Indexing                                                   87\n",
      "Lung                                                          79\n",
      "Calcified Granuloma                                           72\n",
      "Thoracic Vertebrae                                            59\n",
      "                                                            ... \n",
      "Pulmonary Atelectasis;Foreign Bodies;Density                   1\n",
      "Calcified Granuloma;Lung;Markings                              1\n",
      "Lung;Opacity;Markings                                          1\n",
      "Hernia, Diaphragmatic;Bone Diseases, Metabolic;Deformity       1\n",
      "Opacity;Granuloma                                              1\n",
      "Name: count, Length: 1244, dtype: int64\n",
      "findings\n",
      "The heart and lungs have XXXX XXXX in the interval. Both lungs are clear and expanded. Heart and mediastinum normal.                                                                                                                                        51\n",
      "The heart is normal in size. The mediastinum is unremarkable. The lungs are clear.                                                                                                                                                                          51\n",
      "Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.                                                                                                                           46\n",
      "The lungs are clear bilaterally. Specifically, no evidence of focal consolidation, pneumothorax, or pleural effusion.. Cardio mediastinal silhouette is unremarkable. Visualized osseous structures of the thorax are without acute abnormality.            45\n",
      "Cardiac and mediastinal contours are within normal limits. The lungs are clear. Bony structures are intact.                                                                                                                                                 35\n",
      "                                                                                                                                                                                                                                                            ..\n",
      "Lungs are clear. There is no pleural effusion or pneumothorax. The heart and mediastinum are normal. The skeletal structures and soft tissues are normal.                                                                                                    1\n",
      "Low lung volumes are present. The heart size and pulmonary vascularity appear within normal limits. The lungs are free of focal airspace disease. No pleural effusion or pneumothorax is seen. Mild degenerative changes are present in the spine.           1\n",
      "The heart, pulmonary XXXX and mediastinum are within normal limits. There is no pleural effusion or pneumothorax. There is no focal air space opacity to suggest a pneumonia. The patient is obese.                                                          1\n",
      "Heart size within normal limits, stable mediastinal and hilar contours, mediastinal calcifications suggest a previous granulomatous process. No focal alveolar consolidation, no definite pleural effusion seen. No typical findings of pulmonary edema.     1\n",
      "Heart size within normal limits. Small, nodular opacity in the right upper lobe. This does not look like an acute infiltrate, and more XXXX represents a granuloma. No pneumothorax or effusions.                                                            1\n",
      "Name: count, Length: 2553, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Problems'].value_counts())\n",
    "print(df['findings'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limiting 100 characters for pipeline checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['findings'] = df['findings'].str[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Problems'] = df['Problems'].str.replace(';', ' and ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized source has shape torch.Size([2669, 128])\n",
      "Tokenized target has shape torch.Size([2669, 128])\n",
      "Eval source has shape torch.Size([668, 128])\n",
      "Eval target has shape torch.Size([668, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting source and target into train and eval sets (80-20 split)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "source = df['findings'].tolist()\n",
    "target = df['findings'].tolist()\n",
    "\n",
    "source_train, source_eval, target_train, target_eval = train_test_split(\n",
    "    source, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the data into strings\n",
    "# source_train_str = '\\n'.join(source_train)\n",
    "# target_train_str = '\\n'.join(target_train)\n",
    "# source_eval_str = '\\n'.join(source_eval)\n",
    "# target_eval_str = '\\n'.join(target_eval)\n",
    "\n",
    "max_length = 128  # or any other desired maximum length\n",
    "# Initialize the T5 tokenizer\n",
    "# Initialize the BioClinicalBERT tokenizer\n",
    "from transformers import AutoTokenizer, BioGptForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the source and target texts with max_length\n",
    "tokenized_source = tokenizer(source_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "tokenized_target = tokenizer(target_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "\n",
    "eval_source = tokenizer(source_eval, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "eval_target = tokenizer(target_eval, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "\n",
    "print('Tokenized source has shape', tokenized_source['input_ids'].shape)\n",
    "print('Tokenized target has shape', tokenized_target['input_ids'].shape)\n",
    "\n",
    "print('Eval source has shape', eval_source['input_ids'].shape)\n",
    "print('Eval target has shape', eval_target['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,    18,  3800,  ...,     1,     1,     1],\n",
      "        [    2,  5389,     8,  ...,     1,     1,     1],\n",
      "        [    2,    18,  6439,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2, 12518,  1344,  ...,     1,     1,     1],\n",
      "        [    2, 15137,  1419,  ...,     1,     1,     1],\n",
      "        [    2,   271,    31,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[    2,    18,  3800,  ...,     1,     1,     1],\n",
      "        [    2,  5389,     8,  ...,     1,     1,     1],\n",
      "        [    2,    18,  6439,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2, 12518,  1344,  ...,     1,     1,     1],\n",
      "        [    2, 15137,  1419,  ...,     1,     1,     1],\n",
      "        [    2,   271,    31,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_source)\n",
    "print(tokenized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, source_input_ids, source_attention_mask, target_input_ids, target_attention_mask):\n",
    "        self.source_input_ids = source_input_ids\n",
    "        self.source_attention_mask = source_attention_mask\n",
    "        self.target_input_ids = target_input_ids\n",
    "        self.target_attention_mask = target_attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.source_input_ids[idx],\n",
    "            'attention_mask': self.source_attention_mask[idx],\n",
    "            'labels': self.target_input_ids[idx],\n",
    "            'labels_attention_mask': self.target_attention_mask[idx]\n",
    "        }\n",
    "\n",
    "# Assuming you have the tokenized tensors: tokenized_source and tokenized_target\n",
    "train_dataset = CustomDataset(\n",
    "    tokenized_source['input_ids'],\n",
    "    tokenized_source['attention_mask'],\n",
    "    tokenized_target['input_ids'],\n",
    "    tokenized_target['attention_mask']\n",
    ")\n",
    "eval_dataset = CustomDataset(\n",
    "    eval_source['input_ids'],\n",
    "    eval_source['attention_mask'],\n",
    "    eval_target['input_ids'],\n",
    "    eval_target['attention_mask']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x00000254FD6D6A10>\n",
      "<__main__.CustomDataset object at 0x00000254FF67CD50>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Source Text Tokenized:\n",
      "['</s>', 'The</w>', 'lungs</w>', 'are</w>', 'hyperex', 'p', 'anded</w>', ',</w>', 'consistent</w>', 'with</w>', 'COPD</w>', '.</w>', 'Mild</w>', 'cardi', 'omegaly</w>', '.</w>', 'No</w>', 'focal</w>', 'lung</w>', 'consolidation</w>', '.</w>', 'No</w>', 'pneumothorax</w>', 'or</w>', 'pleural</w>', 'effusion</w>', '.</w>', 'Pulmonary</w>', 'vascularity</w>', 'is</w>', 'within</w>', 'normal</w>', 'limits</w>', '.</w>', 'Mild</w>', 'degenerative</w>', 'changes</w>', 'of</w>', 'the</w>', 'thoracic</w>', 'spine</w>', '.</w>', 'Aortic</w>', 'calcifications</w>', 'consistent</w>', 'with</w>', 'atherosclerotic</w>', 'disease</w>', '.</w>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Sample Target Text Tokenized:\n",
      "['</s>', 'The</w>', 'lungs</w>', 'are</w>', 'hyperex', 'p', 'anded</w>', ',</w>', 'consistent</w>', 'with</w>', 'COPD</w>', '.</w>', 'Mild</w>', 'cardi', 'omegaly</w>', '.</w>', 'No</w>', 'focal</w>', 'lung</w>', 'consolidation</w>', '.</w>', 'No</w>', 'pneumothorax</w>', 'or</w>', 'pleural</w>', 'effusion</w>', '.</w>', 'Pulmonary</w>', 'vascularity</w>', 'is</w>', 'within</w>', 'normal</w>', 'limits</w>', '.</w>', 'Mild</w>', 'degenerative</w>', 'changes</w>', 'of</w>', 'the</w>', 'thoracic</w>', 'spine</w>', '.</w>', 'Aortic</w>', 'calcifications</w>', 'consistent</w>', 'with</w>', 'atherosclerotic</w>', 'disease</w>', '.</w>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Print out tokens to see if they look correct\n",
    "sample_idx = 12  # Example index, you can change it to view different samples\n",
    "\n",
    "print(\"Sample Source Text Tokenized:\")\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_source['input_ids'][sample_idx]))\n",
    "\n",
    "print(\"\\nSample Target Text Tokenized:\")\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_target['input_ids'][sample_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModelForTokenClassification\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Initialize the BioClinicalBERT model\n",
    "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4558cc1ea1af4ff6b450a661d46828dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8853, 'learning_rate': 1e-05, 'epoch': 0.3}\n",
      "{'loss': 1.1273, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5702, 'learning_rate': 3e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75389f656a474ab9ab522fd194645166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4948144853115082, 'eval_runtime': 18.305, 'eval_samples_per_second': 36.493, 'eval_steps_per_second': 4.589, 'epoch': 1.0}\n",
      "{'loss': 0.4803, 'learning_rate': 4e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4647, 'learning_rate': 5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.4293, 'learning_rate': 4.8891352549889134e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2411863876b448238ca1f654d59525d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44419065117836, 'eval_runtime': 15.4879, 'eval_samples_per_second': 43.13, 'eval_steps_per_second': 5.424, 'epoch': 2.0}\n",
      "{'loss': 0.3751, 'learning_rate': 4.778270509977827e-05, 'epoch': 2.1}\n",
      "{'loss': 0.3441, 'learning_rate': 4.667405764966741e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3319, 'learning_rate': 4.556541019955654e-05, 'epoch': 2.69}\n",
      "{'loss': 0.3265, 'learning_rate': 4.445676274944568e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec22413a7374c76b848afe4487d6ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.429700642824173, 'eval_runtime': 15.475, 'eval_samples_per_second': 43.166, 'eval_steps_per_second': 5.428, 'epoch': 3.0}\n",
      "{'loss': 0.2704, 'learning_rate': 4.334811529933481e-05, 'epoch': 3.29}\n",
      "{'loss': 0.2515, 'learning_rate': 4.2239467849223955e-05, 'epoch': 3.59}\n",
      "{'loss': 0.2683, 'learning_rate': 4.1130820399113086e-05, 'epoch': 3.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889a7df67e524491a44ee71bbe939175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4416053593158722, 'eval_runtime': 15.5059, 'eval_samples_per_second': 43.08, 'eval_steps_per_second': 5.417, 'epoch': 4.0}\n",
      "{'loss': 0.2244, 'learning_rate': 4.002217294900222e-05, 'epoch': 4.19}\n",
      "{'loss': 0.1941, 'learning_rate': 3.8913525498891355e-05, 'epoch': 4.49}\n",
      "{'loss': 0.2125, 'learning_rate': 3.780487804878049e-05, 'epoch': 4.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a788928d0b4d16befa51e297b27cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47167590260505676, 'eval_runtime': 15.4603, 'eval_samples_per_second': 43.208, 'eval_steps_per_second': 5.433, 'epoch': 5.0}\n",
      "{'loss': 0.1951, 'learning_rate': 3.6696230598669624e-05, 'epoch': 5.09}\n",
      "{'loss': 0.1593, 'learning_rate': 3.558758314855876e-05, 'epoch': 5.39}\n",
      "{'loss': 0.1616, 'learning_rate': 3.447893569844789e-05, 'epoch': 5.69}\n",
      "{'loss': 0.1656, 'learning_rate': 3.337028824833703e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4807ddc01e43fe8f33793b35650cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49640989303588867, 'eval_runtime': 15.5035, 'eval_samples_per_second': 43.087, 'eval_steps_per_second': 5.418, 'epoch': 6.0}\n",
      "{'loss': 0.1311, 'learning_rate': 3.226164079822617e-05, 'epoch': 6.29}\n",
      "{'loss': 0.1295, 'learning_rate': 3.11529933481153e-05, 'epoch': 6.59}\n",
      "{'loss': 0.138, 'learning_rate': 3.0044345898004435e-05, 'epoch': 6.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6455921e16e470bb477873f50f55e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5072482228279114, 'eval_runtime': 15.5212, 'eval_samples_per_second': 43.038, 'eval_steps_per_second': 5.412, 'epoch': 7.0}\n",
      "{'loss': 0.1145, 'learning_rate': 2.8935698447893573e-05, 'epoch': 7.19}\n",
      "{'loss': 0.1113, 'learning_rate': 2.7827050997782704e-05, 'epoch': 7.49}\n",
      "{'loss': 0.1125, 'learning_rate': 2.6718403547671845e-05, 'epoch': 7.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fc0728f8f143ea8f8084235a1fdfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5253611207008362, 'eval_runtime': 15.4916, 'eval_samples_per_second': 43.12, 'eval_steps_per_second': 5.422, 'epoch': 8.0}\n",
      "{'loss': 0.1089, 'learning_rate': 2.5609756097560977e-05, 'epoch': 8.08}\n",
      "{'loss': 0.096, 'learning_rate': 2.4501108647450115e-05, 'epoch': 8.38}\n",
      "{'loss': 0.0997, 'learning_rate': 2.3392461197339246e-05, 'epoch': 8.68}\n",
      "{'loss': 0.0999, 'learning_rate': 2.2283813747228384e-05, 'epoch': 8.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9071eb0d1b4556a64fd841351cf3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.539313793182373, 'eval_runtime': 15.4229, 'eval_samples_per_second': 43.312, 'eval_steps_per_second': 5.446, 'epoch': 9.0}\n",
      "{'loss': 0.0897, 'learning_rate': 2.117516629711752e-05, 'epoch': 9.28}\n",
      "{'loss': 0.0886, 'learning_rate': 2.0066518847006653e-05, 'epoch': 9.58}\n",
      "{'loss': 0.0896, 'learning_rate': 1.8957871396895788e-05, 'epoch': 9.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a438f1908649589871f13b930282a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.549922525882721, 'eval_runtime': 15.4519, 'eval_samples_per_second': 43.231, 'eval_steps_per_second': 5.436, 'epoch': 10.0}\n",
      "{'loss': 0.0857, 'learning_rate': 1.7849223946784922e-05, 'epoch': 10.18}\n",
      "{'loss': 0.082, 'learning_rate': 1.674057649667406e-05, 'epoch': 10.48}\n",
      "{'loss': 0.0821, 'learning_rate': 1.563192904656319e-05, 'epoch': 10.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f5789438d34046908396a7bec6b41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5583472847938538, 'eval_runtime': 15.4598, 'eval_samples_per_second': 43.209, 'eval_steps_per_second': 5.433, 'epoch': 11.0}\n",
      "{'loss': 0.0839, 'learning_rate': 1.452328159645233e-05, 'epoch': 11.08}\n",
      "{'loss': 0.0772, 'learning_rate': 1.3414634146341466e-05, 'epoch': 11.38}\n",
      "{'loss': 0.0787, 'learning_rate': 1.23059866962306e-05, 'epoch': 11.68}\n",
      "{'loss': 0.0803, 'learning_rate': 1.1197339246119735e-05, 'epoch': 11.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b47cefdff5044d0a3eee4450fed2705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5646600127220154, 'eval_runtime': 15.5206, 'eval_samples_per_second': 43.039, 'eval_steps_per_second': 5.412, 'epoch': 12.0}\n",
      "{'loss': 0.0744, 'learning_rate': 1.008869179600887e-05, 'epoch': 12.28}\n",
      "{'loss': 0.0746, 'learning_rate': 8.980044345898004e-06, 'epoch': 12.57}\n",
      "{'loss': 0.0754, 'learning_rate': 7.87139689578714e-06, 'epoch': 12.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc8ab0d0c4f4a7bb9c7749c1a6da0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5696318745613098, 'eval_runtime': 15.5262, 'eval_samples_per_second': 43.024, 'eval_steps_per_second': 5.41, 'epoch': 13.0}\n",
      "{'loss': 0.0723, 'learning_rate': 6.7627494456762755e-06, 'epoch': 13.17}\n",
      "{'loss': 0.0714, 'learning_rate': 5.65410199556541e-06, 'epoch': 13.47}\n",
      "{'loss': 0.0721, 'learning_rate': 4.5454545454545455e-06, 'epoch': 13.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc42eedc3f94294a33e9a2ec8f72241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5746690630912781, 'eval_runtime': 15.4816, 'eval_samples_per_second': 43.148, 'eval_steps_per_second': 5.426, 'epoch': 14.0}\n",
      "{'loss': 0.0717, 'learning_rate': 3.436807095343681e-06, 'epoch': 14.07}\n",
      "{'loss': 0.0681, 'learning_rate': 2.328159645232816e-06, 'epoch': 14.37}\n",
      "{'loss': 0.0692, 'learning_rate': 1.2195121951219514e-06, 'epoch': 14.67}\n",
      "{'loss': 0.0694, 'learning_rate': 1.1086474501108648e-07, 'epoch': 14.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291e392a1e3f4db3a01ad819e2a35e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5771459937095642, 'eval_runtime': 15.45, 'eval_samples_per_second': 43.236, 'eval_steps_per_second': 5.437, 'epoch': 15.0}\n",
      "{'train_runtime': 11134.2677, 'train_samples_per_second': 3.596, 'train_steps_per_second': 0.45, 'train_loss': 0.32219941303163707, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the created dataset here\n",
    "    eval_dataset=eval_dataset\n",
    "    # eval_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model to a directory\n",
    "model.save_pretrained(\"/FYP_DATASET/results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BioGptForSequenceClassification were not initialized from the model checkpoint at /FYP_DATASET/results/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioGptForSequenceClassification(\n",
      "  (biogpt): BioGptModel(\n",
      "    (embed_tokens): Embedding(42384, 1024, padding_idx=1)\n",
      "    (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x BioGptDecoderLayer(\n",
      "        (self_attn): BioGptAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=1024, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"/FYP_DATASET/results/\")\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Input to Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# df= pd.read_csv(\"indiana_reports.csv\")[[\"findings\"]]\n",
    "# df.dropna(subset=['findings'], inplace=True)  # Remove rows with NaN in 'findings'\n",
    "# df = df[df['findings'].str.strip().astype(bool)]  # Remove rows with empty or whitespace 'findings'\n",
    "\n",
    "# all_sentences = []\n",
    "# for finding in df['findings']:\n",
    "#     sentences = finding.split('\\n')  # Split into sentences assuming '.' is the sentence delimiter\n",
    "#     # Filter sentences by length (25 characters) and remove any leading/trailing spaces\n",
    "#     filtered_sentences = [sentence.strip() for sentence in sentences if len(sentence.strip()) == 25]\n",
    "#     all_sentences.extend(filtered_sentences)\n",
    "\n",
    "# # Ensure there are enough sentences of the exact length\n",
    "# if len(all_sentences) >= 5:\n",
    "#     # Randomly select 5 sentences\n",
    "#     random_sentences = np.random.choice(all_sentences, size=5, replace=False)\n",
    "# else:\n",
    "#     print(\"Not enough sentences of exactly 25 characters. Adjusting selection criteria.\")\n",
    "#     # Handle the case where there are not enough sentences of the exact length, e.g., by choosing shorter ones or repeating some\n",
    "#     # This is a placeholder for whatever fallback logic you deem appropriate\n",
    "#     random_sentences = np.random.choice(all_sentences, size=5, replace=True)  # Example fallback\n",
    "\n",
    "\n",
    "# # Load the language model and tokenizer\n",
    "# model_directory = \"/FYP_DATASET/results/\"\n",
    "# loaded_model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "# # Get user input\n",
    "# user_input = random_sentences\n",
    "# # user_input=user_input.replace(';', ' and ')\n",
    "\n",
    "# # Tokenize the user input\n",
    "# input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate output from the language model\n",
    "# outputs = []\n",
    "\n",
    "# for user_input in random_sentences:\n",
    "#     # Tokenize the user input\n",
    "#     input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "#     # Generate output from the language model\n",
    "#     output = loaded_model.generate(\n",
    "#         input_ids,\n",
    "#         max_length=1000,  # Adjust the maximum length here\n",
    "#         num_return_sequences=1,  # Increase if you need multiple sequences\n",
    "#         pad_token_id=tokenizer.eos_token_id,  # Padding token ID\n",
    "#         do_sample=True,  # Enable sampling\n",
    "#         temperature=0.7,  # Control randomness of sampling\n",
    "#         top_k=50,  # Filter top-k tokens to sample from\n",
    "#         top_p=0.95,  # Filter cumulative probability for top-p sampling\n",
    "#         repetition_penalty=1.0,  # Adjusts for repeating tokens\n",
    "#     )\n",
    "\n",
    "#     # Add the generated output to the list\n",
    "#     outputs.append((user_input, output[0]))\n",
    "\n",
    "# # Now, decode and print all outputs\n",
    "# for user_input, output in outputs:\n",
    "#     decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "#     print(\"User input:\", user_input)\n",
    "#     print(\"Generated output:\", decoded_output)\n",
    "#     print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Input to Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "User input: Acute Cardiomegaly\n",
      "Generated output: Acute Cardiomegaly. Mediastinal contours are normal limits. Increased interstitial opacities. No pneumothorax or large pleural effusion. No acute osseous abnormality. cy. TIPA.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the language model and tokenizer\n",
    "model_directory = \"/FYP_DATASET/results/\"\n",
    "\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead. Inference might be slow.\")\n",
    "\n",
    "# Move the model to the GPU\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter your input text: \")\n",
    "user_input=user_input.replace(';', ' and ')\n",
    "\n",
    "# Tokenize the user input\n",
    "input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input_ids to the same device as the model\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "\n",
    "# Generate output from the language model\n",
    "output = loaded_model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,  # Adjust the maximum length here\n",
    "    num_return_sequences=1,  # Increase if you need multiple sequences\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Padding token ID\n",
    "    do_sample=True,  # Enable sampling\n",
    "    temperature=0.7,  # Control randomness of sampling\n",
    "    top_k=50,  # Filter top-k tokens to sample from\n",
    "    top_p=0.95,  # Filter cumulative probability for top-p sampling\n",
    "    repetition_penalty=1.0,  # Adjusts for repeating tokens\n",
    ")\n",
    "\n",
    "# Decode the output tokens to text\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"User input:\", user_input)\n",
    "# Display the generated text\n",
    "print(\"Generated output:\", decoded_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
